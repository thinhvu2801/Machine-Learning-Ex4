{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import cv2\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.color.adapt_rgb import adapt_rgb, each_channel, hsv_value\n",
    "from skimage import filters\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions to convert color space and plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hold the color code name and opencv objects in a dict for easy conversion\n",
    "colorCodeDict = {\n",
    "    'RGB2GRAY' : cv2.COLOR_RGB2GRAY,\n",
    "    'RGB2RGBA' : cv2.COLOR_RGB2RGBA,\n",
    "    'RGB2BGR' : cv2.COLOR_RGB2BGR,\n",
    "    'RGB2BGRA' : cv2.COLOR_RGB2BGRA,\n",
    "    'RGB2HSV' : cv2.COLOR_RGB2HSV,\n",
    "    'RGB2HLS' : cv2.COLOR_RGB2HLS,\n",
    "    'RGB2LUV' : cv2.COLOR_RGB2LUV,\n",
    "    'RGB2YUV' : cv2.COLOR_RGB2YUV,\n",
    "    'RGB2YCrCb' : cv2.COLOR_RGB2YCrCb,\n",
    "    \n",
    "    'BGR2GRAY' : cv2.COLOR_BGR2GRAY,\n",
    "    'BGR2BGRA' : cv2.COLOR_BGR2BGRA,\n",
    "    'BGR2RGB' : cv2.COLOR_BGR2RGB,\n",
    "    'BGR2RGBA' : cv2.COLOR_BGR2RGBA,\n",
    "    'BGR2HSV' : cv2.COLOR_BGR2HSV,\n",
    "    'BGR2HLS' : cv2.COLOR_BGR2HLS,\n",
    "    'BGR2LUV' : cv2.COLOR_RGB2LUV,\n",
    "    'BGR2YUV' : cv2.COLOR_RGB2YUV,\n",
    "    'BGR2YCrCb' : cv2.COLOR_RGB2YCrCb\n",
    "}\n",
    "\n",
    "def convert_color(img , convCode='RGB2GRAY'):\n",
    "    \"\"\"\n",
    "        return image converted to required colospace\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(img, colorCodeDict[convCode]);\n",
    "\n",
    "def plot_img(img, show_stages=False, label=\"\"):\n",
    "    \"\"\"\n",
    "        plot image\n",
    "    \"\"\"\n",
    "    if show_stages:\n",
    "        print(\"############################# \"+ label +\" ##################################\")\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to extract different image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    \"\"\"\n",
    "        Return the image color bins\n",
    "    \"\"\"\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "                        \n",
    "def color_hist(img, nbins=32):\n",
    "    \"\"\"\n",
    "        Return all channel histogram.\n",
    "    \"\"\"\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    return np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, feature_vector=True, visualize=False):\n",
    "    \"\"\"\n",
    "    Return a histogram of oriented gradients using skimage.\n",
    "    \"\"\"\n",
    "    return hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "               cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "               visualize=visualize, feature_vector=feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function to extract and combine different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(images, colorConv, orient, pix_per_cell, cell_per_block, hog_channel):\n",
    "    hog_features = []\n",
    "    for image in images:\n",
    "        # Đọc và xử lý hình ảnh ở đây\n",
    "        feature_image = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if hog_channel == 'ALL':\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], orient, \n",
    "                                                      pix_per_cell, cell_per_block, visualize=False))\n",
    "        else:\n",
    "            hog_features.append(get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                                                  pix_per_cell, cell_per_block, visualize=False))\n",
    "    return np.array(hog_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train and test different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_SVC(X_train, y_train):\n",
    "    \"\"\"\n",
    "        Function to train an svm.\n",
    "    \"\"\"\n",
    "    svc = svm.LinearSVC()\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "    return svc\n",
    "\n",
    "def train_dtree(X_train, y_train):\n",
    "    \"\"\"\n",
    "        Function to train a decision tree.\n",
    "    \"\"\"\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    t=time.time()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    t=time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train dtree...')\n",
    "    return clf\n",
    "\n",
    "def test_classifier(svc, X_test, y_test):\n",
    "    \"\"\"\n",
    "        Funtion to test the classifier.\n",
    "    \"\"\"\n",
    "    print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "    # Check the prediction time for a single sample\n",
    "    t=time.time()\n",
    "    n_predict = 10\n",
    "    pred = svc.predict(X_test[0:n_predict])\n",
    "    actual = y_test[0:n_predict]\n",
    "    print('My SVC predicts: ', pred)\n",
    "    print('For these',n_predict, 'labels: ', actual)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to remove duplicate detections and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    \"\"\"\n",
    "        Iterate the windows with detected cars and enhance the once with highest detections.\n",
    "    \"\"\"\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    \"\"\"\n",
    "        Only keep the detections that have a minimum number of pixels.\n",
    "    \"\"\"\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap < threshold] = 0\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    \"\"\"\n",
    "        Draw the boxes on the detected cars\n",
    "    \"\"\"\n",
    "    for i in range(1, labels[1]+1):\n",
    "        # Find pixels with each car label value\n",
    "        nonzero = (labels[0] == i).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the classifier to detect cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cars(img, colorConv, svc, X_scaler, orient, pix_per_cell, cell_per_block):\n",
    "    \"\"\"\n",
    "        This function takes in an image, extracts the features from a region of interest and\n",
    "        runs the predictions on the features.\n",
    "        Returns a list of co-ordinates where car is detected.\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32)/255\n",
    "    img_shape = img.shape\n",
    "    # Crop the image to remove sky and car bonnet\n",
    "    ystart = math.floor(img_shape[0]*.55)\n",
    "    ystop = math.floor(img_shape[0]*.85)\n",
    "    img = img[ystart:ystop,:,:]\n",
    "    #plot_img(img_tosearch, True)\n",
    "    img = convert_color(img, colorConv)\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (img.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (img.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    # set the window size same as the test image size\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    cells_per_step = 2\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog_ch1 = get_hog_features(img[:,:,0], orient, pix_per_cell, cell_per_block, feature_vector=False)\n",
    "    hog_ch2 = get_hog_features(img[:,:,1], orient, pix_per_cell, cell_per_block, feature_vector=False)\n",
    "    hog_ch3 = get_hog_features(img[:,:,2], orient, pix_per_cell, cell_per_block, feature_vector=False)\n",
    "    on_windows = []\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog_ch1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog_ch2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog_ch3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(img[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=(32, 32))\n",
    "            # Get histogram feature\n",
    "            hist_features = color_hist(subimg, nbins=32)\n",
    "            # add all features and Scale them\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            # make a prediction\n",
    "            test_prediction = svc.predict(test_features)\n",
    "            # Add to list of windows if car predicted\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft)\n",
    "                ytop_draw = np.int(ytop)\n",
    "                win_draw = np.int(window)\n",
    "                on_windows.append(((xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart)))\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set prameters to be used for training the classifier and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colorConv = 'BGR2HSV'\n",
    "hog_channel = \"ALL\"\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "recent_heatmaps = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Number of training examples = 42624\n",
      "Number of testing examples = 10656\n"
     ]
    }
   ],
   "source": [
    "def setup_train_data(colorConv, orient, pix_per_cell, cell_per_block, hog_channel):\n",
    "    \"\"\"\n",
    "        Setup data for classifier training. \n",
    "        Shuffle the data and split it in training and testing set.\n",
    "    \"\"\"\n",
    "    cars = []\n",
    "    images = glob.glob('C:/DISK D/CCTCC/MachineLearning/object-detection-with-svm-and-opencv/data/vehicles/*.png', recursive=True)\n",
    "    for image in images:\n",
    "        cars.append(image)\n",
    "    \n",
    "    images = glob.glob('C:/DISK D/CCTCC/MachineLearning/object-detection-with-svm-and-opencv/data/non-vehicles/*.png', recursive=True)\n",
    "    notcars = []\n",
    "    for image in images:\n",
    "        notcars.append(image)\n",
    "\n",
    "    car_features = extract_features(cars, colorConv, orient=orient, \n",
    "                                pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                                hog_channel=hog_channel)\n",
    "    notcar_features = extract_features(notcars, colorConv, orient=orient, \n",
    "                            pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel)\n",
    "    # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    # Define the labels vector\n",
    "    y_train = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "    # shuffle the data\n",
    "    #X_train, y_train = shuffle(scaled_X, y_train)\n",
    "    # Split up data into randomized training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y_train, test_size=0.2, random_state=2)\n",
    "    return X_train, X_test, y_train, y_test, X_scaler\n",
    "\n",
    "print('Preparing training data...')\n",
    "X_train, X_test, y_train, y_test, X_scaler = setup_train_data(colorConv, orient, \n",
    "                                                              pix_per_cell, cell_per_block, hog_channel)\n",
    "print(\"Number of training examples =\", len(X_train))\n",
    "print(\"Number of testing examples =\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "24.04 Seconds to train SVC...\n"
     ]
    }
   ],
   "source": [
    "print('Training Classifier...')\n",
    "svc = train_SVC(X_train, y_train)\n",
    "#clf = train_dtree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classifier...\n",
      "Test Accuracy of SVC =  0.9592\n",
      "My SVC predicts:  [0. 1. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "For these 10 labels:  [0. 1. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
      "0.002 Seconds to predict 10 labels with SVC\n"
     ]
    }
   ],
   "source": [
    "print('Testing Classifier...')\n",
    "test_classifier(svc, X_test, y_test)\n",
    "#test_classifier(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wraper function to preprocess image run the classifier and post processing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_img(img, heat_threshold=1, show_stages=False):\n",
    "    \"\"\"\n",
    "    Wrapper function to perform all the processing.\n",
    "    \"\"\"\n",
    "    # get the windows where the classifier predicts car\n",
    "    hot_windows = find_cars(img, colorConv, svc, X_scaler, orient, pix_per_cell, cell_per_block)\n",
    "    if show_stages:\n",
    "        img1 = np.copy(img)\n",
    "\n",
    "    # Nếu có windows nóng (hot windows)\n",
    "    if len(hot_windows) > 0:\n",
    "        # Vẽ các cửa sổ trên hình ảnh\n",
    "        for window in hot_windows:\n",
    "            cv2.rectangle(img1, (window[0], window[1]), (window[2], window[3]), (0, 255, 0), 6)\n",
    "\n",
    "    # Thêm đoạn mã tính toán đặc trưng cho subimg\n",
    "    subimg = img[window[1]:window[3], window[0]:window[2]]  # Cắt vùng của cửa sổ\n",
    "    spatial_features = bin_spatial(subimg, size=(32, 32))\n",
    "    hist_features = color_hist(subimg, nbins=32)\n",
    "    hog_features = get_hog_features(subimg, orient, pix_per_cell, cell_per_block, feature_vector=True)\n",
    "\n",
    "    # Gộp tất cả các đặc trưng\n",
    "    test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))\n",
    "\n",
    "    # Trả về hình ảnh đã được xử lý\n",
    "    return img1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------test_images\\test.jpg---------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8460 features, but StandardScaler is expecting 1764 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m recent_heatmaps \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m mpimg\u001b[38;5;241m.\u001b[39mimread(test_image)\n\u001b[1;32m----> 8\u001b[0m out_img \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plot_img(out_img, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn[70], line 6\u001b[0m, in \u001b[0;36mprocess_img\u001b[1;34m(img, heat_threshold, show_stages)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mWrapper function to perform all the processing.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# get the windows where the classifier predicts car\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m hot_windows \u001b[38;5;241m=\u001b[39m \u001b[43mfind_cars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolorConv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpix_per_cell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_per_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_stages:\n\u001b[0;32m      8\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(img)\n",
      "Cell \u001b[1;32mIn[65], line 48\u001b[0m, in \u001b[0;36mfind_cars\u001b[1;34m(img, colorConv, svc, X_scaler, orient, pix_per_cell, cell_per_block)\u001b[0m\n\u001b[0;32m     46\u001b[0m hist_features \u001b[38;5;241m=\u001b[39m color_hist(subimg, nbins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# add all features and Scale them\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m test_features \u001b[38;5;241m=\u001b[39m \u001b[43mX_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspatial_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhist_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhog_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# make a prediction\u001b[39;00m\n\u001b[0;32m     50\u001b[0m test_prediction \u001b[38;5;241m=\u001b[39m svc\u001b[38;5;241m.\u001b[39mpredict(test_features)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\.conda\\envs\\py312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Hi\\.conda\\envs\\py312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\Hi\\.conda\\envs\\py312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Hi\\.conda\\envs\\py312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8460 features, but StandardScaler is expecting 1764 features as input."
     ]
    }
   ],
   "source": [
    "test_dir = \"test_images/\"\n",
    "test_images = glob.glob(test_dir+'tes*.jpg')\n",
    "for test_image in test_images:\n",
    "    print()\n",
    "    print(\"------------------------------\"+test_image+\"---------------------------------\")\n",
    "    recent_heatmaps = deque(maxlen=10)\n",
    "    img = mpimg.imread(test_image)\n",
    "    out_img = process_img(img, 1, True)\n",
    "    plot_img(out_img, True, \"Final Result\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video project_video_res.mp4\n",
      "[MoviePy] Writing video project_video_res.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [14:21<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project_video_res.mp4 \n",
      "\n",
      "[MoviePy] >>>> Building video test_video_res.mp4\n",
      "[MoviePy] Writing video test_video_res.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 38/39 [00:25<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_video_res.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "recent_heatmaps = deque(maxlen=10)\n",
    "project_video_res = 'project_video_res.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project_video_clip = clip1.fl_image(process_img)\n",
    "project_video_clip.write_videofile(project_video_res, audio=False)\n",
    "\n",
    "recent_heatmaps = deque(maxlen=10)\n",
    "project_video_res = 'test_video_res.mp4'\n",
    "clip1 = VideoFileClip(\"test_video.mp4\")\n",
    "project_video_clip = clip1.fl_image(process_img)\n",
    "project_video_clip.write_videofile(project_video_res, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
